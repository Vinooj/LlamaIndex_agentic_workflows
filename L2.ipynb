{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "def18024-676f-4733-b672-035365a0fcad",
      "metadata": {
        "id": "def18024-676f-4733-b672-035365a0fcad"
      },
      "source": [
        "# Lesson 2: Building a Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bad755-8d8c-4392-bcc0-dd7878314cee",
      "metadata": {
        "id": "64bad755-8d8c-4392-bcc0-dd7878314cee"
      },
      "source": [
        "**Lesson objective**: Use event-based Workflows to define the flow of information around a system\n",
        "\n",
        "In this lab, you'll learn about Workflow concepts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Click the Directory icon of the left and upload the requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "kzbh-qO7Rlej",
        "outputId": "794ad572-9fae-43c4-c832-d3c97c60fbf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kzbh-qO7Rlej",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-core (from -r requirements.txt (line 6))\n",
            "  Downloading llama_index_core-0.12.23.post2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-utils-workflow (from -r requirements.txt (line 7))\n",
            "  Downloading llama_index_utils_workflow-0.3.0-py3-none-any.whl.metadata (665 bytes)\n",
            "Collecting llama-index-llms-openai (from -r requirements.txt (line 8))\n",
            "  Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse (from -r requirements.txt (line 9))\n",
            "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting llama-index-embeddings-openai (from -r requirements.txt (line 10))\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-readers-whisper (from -r requirements.txt (line 11))\n",
            "  Downloading llama_index_readers_whisper-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting gradio (from -r requirements.txt (line 12))\n",
            "  Downloading gradio-5.20.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 6)) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (3.11.13)\n",
            "Collecting dataclasses-json (from llama-index-core->-r requirements.txt (line 6))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core->-r requirements.txt (line 6))\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core->-r requirements.txt (line 6))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core->-r requirements.txt (line 6))\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core->-r requirements.txt (line 6))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core->-r requirements.txt (line 6)) (1.17.2)\n",
            "Collecting pyvis<0.4.0,>=0.3.2 (from llama-index-utils-workflow->-r requirements.txt (line 7))\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai->-r requirements.txt (line 8)) (1.61.1)\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse->-r requirements.txt (line 9))\n",
            "  Downloading llama_cloud_services-0.6.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 12)) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 12)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 12)) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 12)) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 12)) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 12)) (2.2.2)\n",
            "Collecting pydub (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio->-r requirements.txt (line 12)) (0.15.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r requirements.txt (line 12))\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio->-r requirements.txt (line 12)) (14.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 6)) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 6)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 6)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 6)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core->-r requirements.txt (line 6)) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 12)) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core->-r requirements.txt (line 6)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core->-r requirements.txt (line 6)) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core->-r requirements.txt (line 6)) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio->-r requirements.txt (line 12)) (3.17.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse->-r requirements.txt (line 9)) (8.1.8)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.14 (from llama-cloud-services>=0.6.4->llama-parse->-r requirements.txt (line 9))\n",
            "  Downloading llama_cloud-0.1.14-py3-none-any.whl.metadata (902 bytes)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse->-r requirements.txt (line 9))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core->-r requirements.txt (line 6)) (2024.11.6)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai->-r requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai->-r requirements.txt (line 8)) (0.8.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 12)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 12)) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core->-r requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core->-r requirements.txt (line 6)) (2.27.2)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core->-r requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 12)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 12)) (13.9.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core->-r requirements.txt (line 6))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core->-r requirements.txt (line 6))\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (4.9.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio->-r requirements.txt (line 12)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 12)) (3.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 12)) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis<0.4.0,>=0.3.2->llama-index-utils-workflow->-r requirements.txt (line 7)) (0.2.13)\n",
            "Downloading llama_index_core-0.12.23.post2-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_utils_workflow-0.3.0-py3-none-any.whl (2.8 kB)\n",
            "Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl (16 kB)\n",
            "Downloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_readers_whisper-0.1.0-py3-none-any.whl (3.3 kB)\n",
            "Downloading gradio-5.20.1-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading llama_cloud_services-0.6.5-py3-none-any.whl (28 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading llama_cloud-0.1.14-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, filetype, dirtyjson, uvicorn, tomlkit, semantic-version, ruff, python-multipart, python-dotenv, mypy-extensions, marshmallow, markupsafe, jedi, groovy, ffmpy, aiofiles, typing-inspect, tiktoken, starlette, safehttpx, pyvis, llama-cloud, gradio-client, fastapi, dataclasses-json, llama-index-core, gradio, llama-index-utils-workflow, llama-index-readers-whisper, llama-index-llms-openai, llama-index-embeddings-openai, llama-cloud-services, llama-parse\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 dataclasses-json-0.6.7 dirtyjson-1.0.8 fastapi-0.115.11 ffmpy-0.5.0 filetype-1.2.0 gradio-5.20.1 gradio-client-1.7.2 groovy-0.1.2 jedi-0.19.2 llama-cloud-0.1.14 llama-cloud-services-0.6.5 llama-index-core-0.12.23.post2 llama-index-embeddings-openai-0.3.1 llama-index-llms-openai-0.3.25 llama-index-readers-whisper-0.1.0 llama-index-utils-workflow-0.3.0 llama-parse-0.6.4.post1 markupsafe-2.1.5 marshmallow-3.26.1 mypy-extensions-1.0.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.20 pyvis-0.3.2 ruff-0.9.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tiktoken-0.9.0 tomlkit-0.13.2 typing-inspect-0.9.0 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f8de14",
      "metadata": {
        "id": "42f8de14"
      },
      "source": [
        "<div style=\"background-color:#fff1d7; padding:15px;\"> <b> Note</b>: Make sure to run the notebook cell by cell. Please try to avoid running all cells at once.</div>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your utilities or helper functions to this file.\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# these expect to find a .env file at the directory above the lesson.\n",
        "# the format for that file is (without the comment)\n",
        "#API_KEYNAME=AStringThatIsTheLongAPIKeyFromSomeService\n",
        "def load_env():\n",
        "    _ = load_dotenv(find_dotenv())\n",
        "\n",
        "def get_openai_api_key():\n",
        "    load_env()\n",
        "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    return openai_api_key\n",
        "\n",
        "def get_llama_cloud_api_key():\n",
        "    load_env()\n",
        "    llama_cloud_api_key = os.getenv(\"LLAMA_CLOUD_API_KEY\")\n",
        "    return llama_cloud_api_key\n",
        "\n",
        "def extract_html_content(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            html_content = file.read()\n",
        "            html_content = f\"\"\" <div style=\"width: 100%; height: 800px; overflow: hidden;\"> {html_content} </div>\"\"\"\n",
        "            return html_content\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error reading file: {str(e)}\")"
      ],
      "metadata": {
        "id": "nRZCkfymPgYa",
        "outputId": "2fa6b15b-6ece-40bc-b7d6-4b944dd5c53a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "id": "nRZCkfymPgYa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1bfa3163d4a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdotenv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfind_dotenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# these expect to find a .env file at the directory above the lesson.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a395e553-d1f1-4819-9a9b-162d6dc9cfc0",
      "metadata": {
        "height": 81,
        "id": "a395e553-d1f1-4819-9a9b-162d6dc9cfc0",
        "outputId": "fe62b6f0-92dd-4b4c-cec4-b73d3136500e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'extract_html_content' from 'helper' (unknown location)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-702744aa26cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_html_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_openai_api_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'extract_html_content' from 'helper' (unknown location)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from IPython.display import display, HTML\n",
        "from helper import extract_html_content\n",
        "import random\n",
        "from helper import get_openai_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80f1e17e-362a-41b5-a002-4d29b4aeca32",
      "metadata": {
        "height": 30,
        "id": "80f1e17e-362a-41b5-a002-4d29b4aeca32",
        "outputId": "1902b402-0a7c-4dd1-f471-15ea42c45f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_openai_api_key' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-cd2fa126bc35>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_openai_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_openai_api_key' is not defined"
          ]
        }
      ],
      "source": [
        "api_key = get_openai_api_key()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e7a7f7",
      "metadata": {
        "id": "96e7a7f7"
      },
      "source": [
        "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
        "<p> ğŸ’» &nbsp; <b>To access <code>requirements.txt</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>.\n",
        "\n",
        "<p> â¬‡ &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
        "\n",
        "<p> ğŸ“’ &nbsp; For more help, please see the <em>\"Appendix â€“ Tips and Help\"</em> Lesson.</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58295bb8",
      "metadata": {
        "id": "58295bb8"
      },
      "source": [
        "<p style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> ğŸš¨\n",
        "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ from those shown in the video.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7da1d6c3-1026-4800-8add-9ad555e94310",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "7da1d6c3-1026-4800-8add-9ad555e94310"
      },
      "source": [
        "## Creating a Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c7ddba6-ce8a-4862-82b0-d28fc7719a9a",
      "metadata": {
        "id": "2c7ddba6-ce8a-4862-82b0-d28fc7719a9a"
      },
      "source": [
        "Under the hood, Workflows are regular Python classes. They are defined as a series of steps, each of which receives certain classes of events and emits certain classes of events.\n",
        "\n",
        "Here's the most basic form of a workflow, with a single step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7869015c-7d6e-4bc4-9f0c-34bdb221a974",
      "metadata": {
        "height": 132,
        "id": "7869015c-7d6e-4bc4-9f0c-34bdb221a974"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.workflow import (\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        "    Context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa53423-2c92-403f-b190-b4ee646870dd",
      "metadata": {
        "height": 115,
        "id": "caa53423-2c92-403f-b190-b4ee646870dd"
      },
      "outputs": [],
      "source": [
        "class MyWorkflow(Workflow):\n",
        "    # declare a function as a step\n",
        "    @step\n",
        "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
        "        # do something here\n",
        "        return StopEvent(result=\"Hello, world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1f4f05d-7407-4161-b5e7-e388e727f44f",
      "metadata": {
        "id": "f1f4f05d-7407-4161-b5e7-e388e727f44f"
      },
      "source": [
        "This new `MyWorkflow` class:\n",
        "* Uses the `@step` decorator to declare a function to be a step\n",
        "* Has a single step called `my_step` which accepts a `StartEvent`. `StartEvent` is a special event which is always generated when a workflow first runs.\n",
        "* `my_step` returns a `StopEvent`, which is another special event. When a `StopEvent` is emitted the workflow returns it and stops running.\n",
        "\n",
        "*Note*: The **async** keyword defines asynchronous functions, which can be paused and resumed, allowing other tasks to run in the meantime.\n",
        "\n",
        "You instantiate it and run your workflow like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64df86f7-5d90-41db-a283-9b81476dbff2",
      "metadata": {
        "height": 98,
        "id": "64df86f7-5d90-41db-a283-9b81476dbff2"
      },
      "outputs": [],
      "source": [
        "# instantiate the workflow\n",
        "basic_workflow = MyWorkflow(timeout=10, verbose=False)\n",
        "# run the workflow\n",
        "result = await basic_workflow.run()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c547af9-a4e7-4b10-913c-6bdacc2c2bf8",
      "metadata": {
        "id": "2c547af9-a4e7-4b10-913c-6bdacc2c2bf8"
      },
      "source": [
        "*Note*:\n",
        "- The **await** keyword is used with async functions to pause execution until the specific asynchronous task is complete.\n",
        "- The **timeout** argument represent the number of seconds after which the workflow execution will be halted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f16a02fc-79c0-471b-8ccd-f25b914f3754",
      "metadata": {
        "id": "f16a02fc-79c0-471b-8ccd-f25b914f3754"
      },
      "source": [
        "### Side Note"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95eaef1b-ac38-4255-9e39-bbded23ce17f",
      "metadata": {
        "id": "95eaef1b-ac38-4255-9e39-bbded23ce17f"
      },
      "source": [
        "Workflows are async by default, so you use `await` to get the result of the `run` command. This will work fine in a notebook environment; in a vanilla python script you will need to import `asyncio` and wrap your code in an async function, like this:\n",
        "\n",
        "```\n",
        "async def main():\n",
        "    w = MyWorkflow(timeout=10, verbose=False)\n",
        "    result = await w.run()\n",
        "    print(result)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(main())\n",
        "```\n",
        "\n",
        "Since you're in a notebook right now, you won't execute the above code as it won't work!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8922ab2-a783-46d7-8c1b-73b6e370ca66",
      "metadata": {
        "id": "d8922ab2-a783-46d7-8c1b-73b6e370ca66"
      },
      "source": [
        "## Visualizing a workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec71ab6-efb7-4740-8906-bb7d2f0ad509",
      "metadata": {
        "id": "6ec71ab6-efb7-4740-8906-bb7d2f0ad509"
      },
      "source": [
        "A great feature of workflows is the built-in visualizer, which you will install now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128fc0a3-2e07-433f-96a9-1a3b123e424d",
      "metadata": {
        "height": 30,
        "id": "128fc0a3-2e07-433f-96a9-1a3b123e424d"
      },
      "outputs": [],
      "source": [
        "from llama_index.utils.workflow import draw_all_possible_flows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "768b444b-095f-4d73-b51d-dc0e51cc635c",
      "metadata": {
        "height": 81,
        "id": "768b444b-095f-4d73-b51d-dc0e51cc635c"
      },
      "outputs": [],
      "source": [
        "draw_all_possible_flows(\n",
        "    basic_workflow,\n",
        "    filename=\"workflows/basic_workflow.html\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853fc336-6bd3-4050-b105-6c0da5df439d",
      "metadata": {
        "id": "853fc336-6bd3-4050-b105-6c0da5df439d"
      },
      "source": [
        "You are provided with a helper function which displays the HTML page generated inside the notebook. On your own computer, you could just open the file in your browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c960b3e-f8e6-4b82-8249-191e876ce503",
      "metadata": {
        "height": 47,
        "id": "9c960b3e-f8e6-4b82-8249-191e876ce503"
      },
      "outputs": [],
      "source": [
        "html_content = extract_html_content(\"workflows/basic_workflow.html\")\n",
        "display(HTML(html_content), metadata=dict(isolated=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07b66b3",
      "metadata": {
        "id": "b07b66b3"
      },
      "source": [
        "<div style=\"background-color:#fff1d7; padding:15px;\"> <b> Note </b>: The visualized workflow might look slightly different from that of the video. If it doesn't display in the notebook, you can: 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. The HTML file is in the folder \"workflows\".</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54381b5b-5007-422b-bbc8-635628add88d",
      "metadata": {
        "id": "54381b5b-5007-422b-bbc8-635628add88d"
      },
      "source": [
        "Of course, a flow with a single step is not very useful! Let's define a multi-step workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f2fe8c-26f8-465b-b099-3744982bb5f8",
      "metadata": {
        "id": "23f2fe8c-26f8-465b-b099-3744982bb5f8"
      },
      "source": [
        "## Creating Custom Events"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19c242a-a249-437c-86a2-9de7f43b4344",
      "metadata": {
        "id": "d19c242a-a249-437c-86a2-9de7f43b4344"
      },
      "source": [
        "Multiple steps are created by defining custom events that can be emitted by steps and trigger other steps. Let's define a simple 3-step workflow by defining two custom events, `FirstEvent` and `SecondEvent`. These classes can have any names and properties, but must inherit from `Event`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42526f9-8726-4b8a-a99e-860c3fb911ed",
      "metadata": {
        "height": 132,
        "id": "f42526f9-8726-4b8a-a99e-860c3fb911ed"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.workflow import Event\n",
        "\n",
        "class FirstEvent(Event):\n",
        "    first_output: str\n",
        "\n",
        "class SecondEvent(Event):\n",
        "    second_output: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f9a8356-379c-4cb7-ad6e-99543fe0f1c0",
      "metadata": {
        "id": "6f9a8356-379c-4cb7-ad6e-99543fe0f1c0"
      },
      "source": [
        "### Defining the workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a7ba9c-2119-463f-bbf5-1207349e9429",
      "metadata": {
        "id": "25a7ba9c-2119-463f-bbf5-1207349e9429"
      },
      "source": [
        "Now you define the workflow itself. You do this by defining the input and output types on each step.\n",
        "\n",
        "* `step_one` takes a `StartEvent` and returns a `FirstEvent`\n",
        "* `step_two` takes a `FirstEvent` and returns a `SecondEvent`\n",
        "* `step_three` takes a `SecondEvent` and returns a `StopEvent`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fb67470-dd94-4a91-b040-e243928a788e",
      "metadata": {
        "height": 268,
        "id": "1fb67470-dd94-4a91-b040-e243928a788e"
      },
      "outputs": [],
      "source": [
        "class MyWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent) -> FirstEvent:\n",
        "        print(ev.first_input)\n",
        "        return FirstEvent(first_output=\"First step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: FirstEvent) -> SecondEvent:\n",
        "        print(ev.first_output)\n",
        "        return SecondEvent(second_output=\"Second step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_three(self, ev: SecondEvent) -> StopEvent:\n",
        "        print(ev.second_output)\n",
        "        return StopEvent(result=\"Workflow complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3b55bd-f41c-44bc-b54f-93cdf51b790c",
      "metadata": {
        "id": "2c3b55bd-f41c-44bc-b54f-93cdf51b790c"
      },
      "source": [
        "*Note:* Properties of `StartEvent` and `StopEvent`:\n",
        "- For `StartEvent`, you define its properties and pass in their values when you run the workflow as shown in the next cell.\n",
        "- For `StopEvent`, by default, it only has one property `result`. You can always create a class that inherits `StopEvent` so you can customize what it returns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f61269cd-0dea-4cb0-9ef8-a248b42dcb8b",
      "metadata": {
        "id": "f61269cd-0dea-4cb0-9ef8-a248b42dcb8b"
      },
      "source": [
        "You run this just like you ran the other workflows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85399e8e-269c-42e4-b0be-1e7f9bdcf67e",
      "metadata": {
        "height": 64,
        "id": "85399e8e-269c-42e4-b0be-1e7f9bdcf67e"
      },
      "outputs": [],
      "source": [
        "workflow = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await workflow.run(first_input=\"Start the workflow.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b02fcb-acb7-40d6-a137-3857bb16a77b",
      "metadata": {
        "id": "30b02fcb-acb7-40d6-a137-3857bb16a77b"
      },
      "source": [
        "And you can visualize it just like you did before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e88ff080-570d-43f3-b189-07108f6902d5",
      "metadata": {
        "height": 47,
        "id": "e88ff080-570d-43f3-b189-07108f6902d5"
      },
      "outputs": [],
      "source": [
        "WORKFLOW_FILE = \"workflows/custom_events.html\"\n",
        "draw_all_possible_flows(workflow, filename=WORKFLOW_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa85bd8-7d70-428e-a19d-a3e1a4ca776d",
      "metadata": {
        "height": 47,
        "id": "aaa85bd8-7d70-428e-a19d-a3e1a4ca776d"
      },
      "outputs": [],
      "source": [
        "html_content = extract_html_content(WORKFLOW_FILE)\n",
        "display(HTML(html_content), metadata=dict(isolated=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610befa4-d934-4035-9a9f-68593b716b7a",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "610befa4-d934-4035-9a9f-68593b716b7a"
      },
      "source": [
        "## Creating Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0651e4e-6aeb-47a5-a028-47c8910521bd",
      "metadata": {
        "id": "a0651e4e-6aeb-47a5-a028-47c8910521bd"
      },
      "source": [
        "However, there's not much point to a workflow if it just runs straight through! A key feature of Workflows is their enablement of branching and looping logic, more simply and flexibly than graph-based approaches. To enable looping, let's create a new `LoopEvent`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2d2c6f-a2f6-4321-986e-7c7c0a01af75",
      "metadata": {
        "height": 47,
        "id": "4b2d2c6f-a2f6-4321-986e-7c7c0a01af75"
      },
      "outputs": [],
      "source": [
        "class LoopEvent(Event):\n",
        "    loop_output: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30efc66f-d647-4582-9db9-c66f2a9e80ed",
      "metadata": {
        "id": "30efc66f-d647-4582-9db9-c66f2a9e80ed"
      },
      "source": [
        "Now you'll edit your `step_one` to make a random decision about whether to execute serially or loop back:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28458e79-7022-4e66-934d-53ea2845283f",
      "metadata": {
        "height": 336,
        "id": "28458e79-7022-4e66-934d-53ea2845283f"
      },
      "outputs": [],
      "source": [
        "class MyWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ev: StartEvent | LoopEvent) -> FirstEvent | LoopEvent:\n",
        "        if random.randint(0, 1) == 0:\n",
        "            print(\"Bad thing happened\")\n",
        "            return LoopEvent(loop_output=\"Back to step one.\")\n",
        "        else:\n",
        "            print(\"Good thing happened\")\n",
        "            return FirstEvent(first_output=\"First step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ev: FirstEvent) -> SecondEvent:\n",
        "        print(ev.first_output)\n",
        "        return SecondEvent(second_output=\"Second step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_three(self, ev: SecondEvent) -> StopEvent:\n",
        "        print(ev.second_output)\n",
        "        return StopEvent(result=\"Workflow complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5ff1e1-1646-4e30-a8e1-36b5bfe245f9",
      "metadata": {
        "id": "9d5ff1e1-1646-4e30-a8e1-36b5bfe245f9"
      },
      "source": [
        "Note the new type annotations on `step_one`: the step now accepts either a `StartEvent` or a `LoopEvent` to trigger the step, and it also emits either a `FirstEvent` or a `LoopEvent`.\n",
        "\n",
        "You run it as usual. You might need to run it a couple of times to see the loop happen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a27c0f-9661-4f06-a8e2-57308056c115",
      "metadata": {
        "height": 64,
        "id": "a9a27c0f-9661-4f06-a8e2-57308056c115"
      },
      "outputs": [],
      "source": [
        "loop_workflow = MyWorkflow(timeout=10, verbose=False)\n",
        "result = await loop_workflow.run(first_input=\"Start the workflow.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c9164a7-767f-4815-bd79-80d60b0eff0a",
      "metadata": {
        "id": "5c9164a7-767f-4815-bd79-80d60b0eff0a"
      },
      "source": [
        "Your new, looping workflow visualizes like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "357849c3-f0eb-4285-a4b7-0b1c5c5205fc",
      "metadata": {
        "height": 81,
        "id": "357849c3-f0eb-4285-a4b7-0b1c5c5205fc"
      },
      "outputs": [],
      "source": [
        "WORKFLOW_FILE = \"workflows/loop_events.html\"\n",
        "draw_all_possible_flows(loop_workflow, filename=WORKFLOW_FILE)\n",
        "html_content = extract_html_content(WORKFLOW_FILE)\n",
        "display(HTML(html_content), metadata=dict(isolated=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdebabcb-e536-4fa0-8354-df1bac208543",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "bdebabcb-e536-4fa0-8354-df1bac208543"
      },
      "source": [
        "## Branching"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b23d594e-c4e4-42c6-a88f-2f8ab7885826",
      "metadata": {
        "id": "b23d594e-c4e4-42c6-a88f-2f8ab7885826"
      },
      "source": [
        "The same constructs that allow you to loop allow you to create branches. Here's a workflow that executes two different branches depending on an early decision:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d378818-6dc1-4994-9729-bc34399b1147",
      "metadata": {
        "height": 200,
        "id": "6d378818-6dc1-4994-9729-bc34399b1147"
      },
      "outputs": [],
      "source": [
        "class BranchA1Event(Event):\n",
        "    payload: str\n",
        "\n",
        "class BranchA2Event(Event):\n",
        "    payload: str\n",
        "\n",
        "class BranchB1Event(Event):\n",
        "    payload: str\n",
        "\n",
        "class BranchB2Event(Event):\n",
        "    payload: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce851bf7-6125-4fbb-9d06-4e30fc3283b1",
      "metadata": {
        "height": 506,
        "id": "ce851bf7-6125-4fbb-9d06-4e30fc3283b1"
      },
      "outputs": [],
      "source": [
        "class BranchWorkflow(Workflow):\n",
        "    @step\n",
        "    async def start(self, ev: StartEvent) -> BranchA1Event | BranchB1Event:\n",
        "        if random.randint(0, 1) == 0:\n",
        "            print(\"Go to branch A\")\n",
        "            return BranchA1Event(payload=\"Branch A\")\n",
        "        else:\n",
        "            print(\"Go to branch B\")\n",
        "            return BranchB1Event(payload=\"Branch B\")\n",
        "\n",
        "    @step\n",
        "    async def step_a1(self, ev: BranchA1Event) -> BranchA2Event:\n",
        "        print(ev.payload)\n",
        "        return BranchA2Event(payload=ev.payload)\n",
        "\n",
        "    @step\n",
        "    async def step_b1(self, ev: BranchB1Event) -> BranchB2Event:\n",
        "        print(ev.payload)\n",
        "        return BranchB2Event(payload=ev.payload)\n",
        "\n",
        "    @step\n",
        "    async def step_a2(self, ev: BranchA2Event) -> StopEvent:\n",
        "        print(ev.payload)\n",
        "        return StopEvent(result=\"Branch A complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_b2(self, ev: BranchB2Event) -> StopEvent:\n",
        "        print(ev.payload)\n",
        "        return StopEvent(result=\"Branch B complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4919c59-75c4-4fa8-a10d-d63f5a3fd1f3",
      "metadata": {
        "id": "f4919c59-75c4-4fa8-a10d-d63f5a3fd1f3"
      },
      "source": [
        "You don't actually need to instantiate the workflow to visualize it, you can just pass the workflow class directly to the visualizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "911c2e4f-3ead-4811-8d63-efd706f2d020",
      "metadata": {
        "height": 64,
        "id": "911c2e4f-3ead-4811-8d63-efd706f2d020"
      },
      "outputs": [],
      "source": [
        "WORKFLOW_FILE = \"workflows/branching.html\"\n",
        "draw_all_possible_flows(BranchWorkflow, filename=WORKFLOW_FILE)\n",
        "html_content = extract_html_content(WORKFLOW_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a64ca9-50b3-4e22-a96f-5f3bfe57bf79",
      "metadata": {
        "height": 30,
        "id": "e7a64ca9-50b3-4e22-a96f-5f3bfe57bf79"
      },
      "outputs": [],
      "source": [
        "display(HTML(html_content), metadata=dict(isolated=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45090b9a-eb7a-4cf6-971a-4014621e5a51",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "45090b9a-eb7a-4cf6-971a-4014621e5a51"
      },
      "source": [
        "## Concurent Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cdd4534-897f-497d-9123-172598073dd0",
      "metadata": {
        "id": "0cdd4534-897f-497d-9123-172598073dd0"
      },
      "source": [
        "The final form of flow control you can implement in workflows is concurrent execution. This allows you to efficiently run long-running tasks in parallel, and gather them together when they are needed. Let's see how this is done.\n",
        "\n",
        "You'll be using a new concept, the `Context` object. This is a form of shared memory available to every step in a workflow: to access it, declare it as an argument to your step and it will be automatically populated.\n",
        "\n",
        "In this example, you'll use `Context.send_event` rather than returning an event. This allows you to emit multiple events in parallel rather than returning just one as you did previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b487882-49a0-42ae-b862-febde9e67e01",
      "metadata": {
        "height": 319,
        "id": "1b487882-49a0-42ae-b862-febde9e67e01"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "class StepTwoEvent(Event):\n",
        "    query: str\n",
        "\n",
        "class ParallelFlow(Workflow):\n",
        "    @step\n",
        "    async def start(self, ctx: Context, ev: StartEvent) -> StepTwoEvent:\n",
        "        ctx.send_event(StepTwoEvent(query=\"Query 1\"))\n",
        "        ctx.send_event(StepTwoEvent(query=\"Query 2\"))\n",
        "        ctx.send_event(StepTwoEvent(query=\"Query 3\"))\n",
        "\n",
        "    @step(num_workers=4)\n",
        "    async def step_two(self, ctx: Context, ev: StepTwoEvent) -> StopEvent:\n",
        "        print(\"Running slow query \", ev.query)\n",
        "        await asyncio.sleep(random.randint(1, 5))\n",
        "\n",
        "        return StopEvent(result=ev.query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec6f9a6-6585-4c7a-9268-a47b52550d37",
      "metadata": {
        "height": 64,
        "id": "8ec6f9a6-6585-4c7a-9268-a47b52550d37"
      },
      "outputs": [],
      "source": [
        "parallel_workflow = ParallelFlow(timeout=10, verbose=False)\n",
        "result = await parallel_workflow.run()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f4f00ab-639f-41ac-896b-59705e57f5d7",
      "metadata": {
        "id": "9f4f00ab-639f-41ac-896b-59705e57f5d7"
      },
      "source": [
        "### Collecting events"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a992084-884b-4a58-bcbd-a7e4b2e14e3d",
      "metadata": {
        "id": "6a992084-884b-4a58-bcbd-a7e4b2e14e3d"
      },
      "source": [
        "But what if you do want the output of all 3 events? Another method, `Context.collect_events`, exists for that purpose:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab2b25fb-5d7d-4a0e-b270-93f03a1e11ab",
      "metadata": {
        "height": 472,
        "id": "ab2b25fb-5d7d-4a0e-b270-93f03a1e11ab"
      },
      "outputs": [],
      "source": [
        "class StepThreeEvent(Event):\n",
        "    result: str\n",
        "\n",
        "class ConcurrentFlow(Workflow):\n",
        "    @step\n",
        "    async def start(self, ctx: Context, ev: StartEvent) -> StepTwoEvent:\n",
        "        ctx.send_event(StepTwoEvent(query=\"Query 1\"))\n",
        "        ctx.send_event(StepTwoEvent(query=\"Query 2\"))\n",
        "        ctx.send_event(StepTwoEvent(query=\"Query 3\"))\n",
        "\n",
        "    @step(num_workers=4)\n",
        "    async def step_two(self, ctx: Context, ev: StepTwoEvent) -> StepThreeEvent:\n",
        "        print(\"Running query \", ev.query)\n",
        "        await asyncio.sleep(random.randint(1, 5))\n",
        "        return StepThreeEvent(result=ev.query)\n",
        "\n",
        "    @step\n",
        "    async def step_three(self, ctx: Context, ev: StepThreeEvent) -> StopEvent:\n",
        "        # wait until we receive 3 events\n",
        "        result = ctx.collect_events(ev, [StepThreeEvent] * 3)\n",
        "        if result is None:\n",
        "            print(\"Not all events received yet.\")\n",
        "            return None\n",
        "\n",
        "        # do something with all 3 results together\n",
        "        print(result)\n",
        "        return StopEvent(result=\"Done\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1048887e-1503-4c64-ad72-ceb0def6ec25",
      "metadata": {
        "height": 64,
        "id": "1048887e-1503-4c64-ad72-ceb0def6ec25"
      },
      "outputs": [],
      "source": [
        "w = ConcurrentFlow(timeout=10, verbose=False)\n",
        "result = await w.run(message=\"Start the workflow.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7586cbc4-c7b3-4f67-a245-38a274ae5cde",
      "metadata": {
        "id": "7586cbc4-c7b3-4f67-a245-38a274ae5cde"
      },
      "source": [
        "What `collect_events` does is store the events in the context until it has collected the number and type of events specified in its second argument. In this case, you've told it to wait for 3 events.\n",
        "\n",
        "If an event fires and `collect_events` hasn't yet seen the right number of events, it returns `None`, so you tell `step_three` to do nothing in that case. When `collect_events` receives the right number of events it returns them as an array, which you can see being printed in the final output.\n",
        "\n",
        "*Note:* This flow control lets you perform map-reduce style tasks.To implement a map-reduce pattern, you would split your task up into as many steps as necessary, and use `Context` to store that number with `ctx.set(\"num_events\", some_number)`. Then in `step_three` you would wait for the number stored in the context using `await ctx.get(\"num_events\")`. So you don't need to know in advance exactly how many concurrent steps you're taking. You'll do exactly this in a later lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6af0be-bedc-4efd-bd33-666f147b7590",
      "metadata": {
        "id": "eb6af0be-bedc-4efd-bd33-666f147b7590"
      },
      "source": [
        "### Collecting different event types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6033c923-8fd6-473e-b771-356fa6676b29",
      "metadata": {
        "id": "6033c923-8fd6-473e-b771-356fa6676b29"
      },
      "source": [
        "You don't just have to wait for multiple events of the same kind. In this example, you'll emit 3 totally different events and collect them at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84dc46d2-7b98-4720-b0c3-5b7b456a48cf",
      "metadata": {
        "height": 302,
        "id": "84dc46d2-7b98-4720-b0c3-5b7b456a48cf"
      },
      "outputs": [],
      "source": [
        "class StepAEvent(Event):\n",
        "    query: str\n",
        "\n",
        "class StepACompleteEvent(Event):\n",
        "    result: str\n",
        "\n",
        "class StepBEvent(Event):\n",
        "    query: str\n",
        "\n",
        "class StepBCompleteEvent(Event):\n",
        "    result: str\n",
        "\n",
        "class StepCEvent(Event):\n",
        "    query: str\n",
        "\n",
        "class StepCCompleteEvent(Event):\n",
        "    result: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c524727b-1d21-4aae-a551-ec115d94d684",
      "metadata": {
        "height": 744,
        "id": "c524727b-1d21-4aae-a551-ec115d94d684"
      },
      "outputs": [],
      "source": [
        "class ConcurrentFlow(Workflow):\n",
        "    @step\n",
        "    async def start(\n",
        "        self, ctx: Context, ev: StartEvent\n",
        "    ) -> StepAEvent | StepBEvent | StepCEvent:\n",
        "        ctx.send_event(StepAEvent(query=\"Query 1\"))\n",
        "        ctx.send_event(StepBEvent(query=\"Query 2\"))\n",
        "        ctx.send_event(StepCEvent(query=\"Query 3\"))\n",
        "\n",
        "    @step\n",
        "    async def step_a(self, ctx: Context, ev: StepAEvent) -> StepACompleteEvent:\n",
        "        print(\"Doing something A-ish\")\n",
        "        return StepACompleteEvent(result=ev.query)\n",
        "\n",
        "    @step\n",
        "    async def step_b(self, ctx: Context, ev: StepBEvent) -> StepBCompleteEvent:\n",
        "        print(\"Doing something B-ish\")\n",
        "        return StepBCompleteEvent(result=ev.query)\n",
        "\n",
        "    @step\n",
        "    async def step_c(self, ctx: Context, ev: StepCEvent) -> StepCCompleteEvent:\n",
        "        print(\"Doing something C-ish\")\n",
        "        return StepCCompleteEvent(result=ev.query)\n",
        "\n",
        "    @step\n",
        "    async def step_three(\n",
        "        self,\n",
        "        ctx: Context,\n",
        "        ev: StepACompleteEvent | StepBCompleteEvent | StepCCompleteEvent,\n",
        "    ) -> StopEvent:\n",
        "        print(\"Received event \", ev.result)\n",
        "\n",
        "        # wait until we receive 3 events\n",
        "        events = ctx.collect_events(\n",
        "            ev,\n",
        "            [StepCCompleteEvent, StepACompleteEvent, StepBCompleteEvent],\n",
        "        )\n",
        "        if (events is None):\n",
        "            return None\n",
        "\n",
        "        # do something with all 3 results together\n",
        "        print(\"All events received: \", events)\n",
        "        return StopEvent(result=\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61901db5-817c-4a75-b0cf-4d1eeab62bc6",
      "metadata": {
        "id": "61901db5-817c-4a75-b0cf-4d1eeab62bc6"
      },
      "source": [
        "When you run it, it will do all three things and wait for them in `step_three`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e36f5c5-1057-47f0-b072-7b82c2478c8b",
      "metadata": {
        "height": 64,
        "id": "2e36f5c5-1057-47f0-b072-7b82c2478c8b"
      },
      "outputs": [],
      "source": [
        "w = ConcurrentFlow(timeout=10, verbose=False)\n",
        "result = await w.run(message=\"Start the workflow.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9702b7bd-7f69-4ef6-a8d2-57460c201693",
      "metadata": {
        "id": "9702b7bd-7f69-4ef6-a8d2-57460c201693"
      },
      "source": [
        "This new flow has quite a pretty visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c893249-c71b-4597-adce-04dea538bf9b",
      "metadata": {
        "height": 64,
        "id": "8c893249-c71b-4597-adce-04dea538bf9b"
      },
      "outputs": [],
      "source": [
        "WORKFLOW_FILE = \"workflows/concurrent_different_events.html\"\n",
        "draw_all_possible_flows(w, filename=WORKFLOW_FILE)\n",
        "html_content = extract_html_content(WORKFLOW_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7822a1d9-febd-4125-8f14-c4a5965a2aa7",
      "metadata": {
        "height": 30,
        "id": "7822a1d9-febd-4125-8f14-c4a5965a2aa7"
      },
      "outputs": [],
      "source": [
        "display(HTML(html_content), metadata=dict(isolated=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5927ba40-53fc-4710-98f2-359928e98f46",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "5927ba40-53fc-4710-98f2-359928e98f46"
      },
      "source": [
        "## Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d12e619-fbed-4fcd-8a12-f16147f16a56",
      "metadata": {
        "id": "6d12e619-fbed-4fcd-8a12-f16147f16a56"
      },
      "source": [
        "In practical use, agents can take a long time to run. It's a poor user-experience to have the user execute a workflow and then wait a long time to see if it works or not; it's better to give them some indication that things are happening in real-time, even if the process is not complete.\n",
        "\n",
        "To do this, Workflows allow streaming events back to the user. Here you'll use `Context.write_event_to_stream` to emit these events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "775c33ea-fcaa-48b1-afe4-0220db6b9bfe",
      "metadata": {
        "height": 30,
        "id": "775c33ea-fcaa-48b1-afe4-0220db6b9bfe"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b32e825f-4539-4992-8b78-bde29ddd1c7a",
      "metadata": {
        "height": 217,
        "id": "b32e825f-4539-4992-8b78-bde29ddd1c7a"
      },
      "outputs": [],
      "source": [
        "class FirstEvent(Event):\n",
        "    first_output: str\n",
        "\n",
        "class SecondEvent(Event):\n",
        "    second_output: str\n",
        "    response: str\n",
        "\n",
        "class TextEvent(Event):\n",
        "    delta: str\n",
        "\n",
        "class ProgressEvent(Event):\n",
        "    msg: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a2079c9-e7eb-4e84-a135-a301a4a016c5",
      "metadata": {
        "id": "9a2079c9-e7eb-4e84-a135-a301a4a016c5"
      },
      "source": [
        "The specific event we'll be sending back is the \"delta\" responses from the LLM. When you ask an LLM to generate a streaming response as you're doing here, it sends back each chunk of its response as it becomes available. This is available as the \"delta\". You're going to wrap the delta in a TextEvent and send it back to the Workflow's own stream."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0e9d392-8a2d-4411-aef2-a5e93102d5d5",
      "metadata": {
        "height": 421,
        "id": "d0e9d392-8a2d-4411-aef2-a5e93102d5d5"
      },
      "outputs": [],
      "source": [
        "class MyWorkflow(Workflow):\n",
        "    @step\n",
        "    async def step_one(self, ctx: Context, ev: StartEvent) -> FirstEvent:\n",
        "        ctx.write_event_to_stream(ProgressEvent(msg=\"Step one is happening\"))\n",
        "        return FirstEvent(first_output=\"First step complete.\")\n",
        "\n",
        "    @step\n",
        "    async def step_two(self, ctx: Context, ev: FirstEvent) -> SecondEvent:\n",
        "        llm = OpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
        "        generator = await llm.astream_complete(\n",
        "            \"Please give me the first 50 words of Moby Dick, a book in the public domain.\"\n",
        "        )\n",
        "        async for response in generator:\n",
        "            # Allow the workflow to stream this piece of response\n",
        "            ctx.write_event_to_stream(TextEvent(delta=response.delta))\n",
        "        return SecondEvent(\n",
        "            second_output=\"Second step complete, full response attached\",\n",
        "            response=str(response),\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def step_three(self, ctx: Context, ev: SecondEvent) -> StopEvent:\n",
        "        ctx.write_event_to_stream(ProgressEvent(msg=\"Step three is happening\"))\n",
        "        return StopEvent(result=\"Workflow complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be37ded4-8587-406a-b730-4082f573a6c2",
      "metadata": {
        "id": "be37ded4-8587-406a-b730-4082f573a6c2"
      },
      "source": [
        "You can work with the emitted events by getting a streaming endpoint from the `run` command, and then filtering it for the types of events we want to see (you could print every event if you wanted to but that would be quite noisy).\n",
        "\n",
        "In this case you'll just print out the progressevents and the textevents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10eaae2e-6950-4192-930d-a1c7cca483c8",
      "metadata": {
        "height": 200,
        "id": "10eaae2e-6950-4192-930d-a1c7cca483c8"
      },
      "outputs": [],
      "source": [
        "workflow = MyWorkflow(timeout=30, verbose=False)\n",
        "handler = workflow.run(first_input=\"Start the workflow.\")\n",
        "\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ProgressEvent):\n",
        "        print(ev.msg)\n",
        "    if isinstance(ev, TextEvent):\n",
        "        print(ev.delta, end=\"\")\n",
        "\n",
        "final_result = await handler\n",
        "print(\"Final result = \", final_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d81176-340f-4dd6-89be-97296d46023e",
      "metadata": {
        "id": "d8d81176-340f-4dd6-89be-97296d46023e"
      },
      "source": [
        "## Congratulations!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87272551-d5a1-4998-af3d-696fa660bbff",
      "metadata": {
        "id": "87272551-d5a1-4998-af3d-696fa660bbff"
      },
      "source": [
        "You've successfully built a number of increasingly-complex Workflows, and mastered the basic concepts. In the next lesson, you'll add RAG to your Workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36a004fa",
      "metadata": {
        "id": "36a004fa"
      },
      "source": [
        "## Resource"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d1f43b8",
      "metadata": {
        "id": "9d1f43b8"
      },
      "source": [
        "[Guide to LlamaIndex's Workflows](https://docs.llamaindex.ai/en/stable/module_guides/workflow/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}